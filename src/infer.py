import torch
import torch.nn.functional as F
from train_test import Transformer  # å¯¼å…¥ä½ çš„Transformeræ¨¡å‹
from data import get_dataloaders  # å¤ç”¨data.pyçš„tokenizeré…ç½®
import os

# ==============================================================================
# ===== å·²æ ¹æ®ä½ çš„è·¯å¾„é…ç½®å®Œæˆï¼Œç›´æ¥è¿è¡Œå³å¯ï¼ =====
# ==============================================================================
# 1. æ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œé€‚é…ä½ çš„ç›®å½•ç»“æ„ï¼‰
MODEL_PATH = "./checkpoints/final_model.pt"  # ä½ çš„æ¨¡å‹è·¯å¾„ï¼šsrc/checkpoints/final_model.pt

# 2. æµ‹è¯•ç”¨çš„å¾·è¯­æ–‡æœ¬ï¼ˆå¯ç›´æ¥ä¿®æ”¹æˆä½ æƒ³æµ‹è¯•çš„å¥å­ï¼‰
TEST_SRC_TEXT = "Heute ist ein wunderschÃ¶ner Tag, und ich freue mich darauf, ins Museum zu gehen."

# 3. æ¨¡å‹å‚æ•°ï¼ˆç”¨train.pyé»˜è®¤å€¼ï¼Œæ²¡ä¿®æ”¹è¿‡å°±ä¸ç”¨åŠ¨ï¼‰
D_MODEL = 512
NUM_HEADS = 8
NUM_ENCODER_LAYERS = 4
NUM_DECODER_LAYERS = 4
D_FF = 2048
MAX_SEQ_LEN = 5000
DROPOUT = 0.1
MAX_GEN_LEN = 128  # æœ€å¤§ç”Ÿæˆé•¿åº¦


# ==============================================================================
# ===== æ ¸å¿ƒé€»è¾‘ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ =====
# ==============================================================================
def create_causal_mask(seq_len, device):
    """åˆ›å»ºå› æœæ©ç ï¼ˆé˜²æ­¢è§£ç æ—¶çœ‹åˆ°æœªæ¥è¯ï¼‰"""
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)
    return mask == 0


def create_padding_mask(seq, pad_idx, device):
    """åˆ›å»ºpaddingæ©ç ï¼ˆå±è”½pad tokenï¼‰"""
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)


def translate_text(
        model,
        src_text,
        src_tokenizer,
        tgt_tokenizer,
        pad_idx,
        max_gen_len=128,
        device="cuda" if torch.cuda.is_available() else "cpu"
):
    """å•å¥ç¿»è¯‘ï¼šå¾·è¯­â†’è‹±è¯­"""
    # 1. æºæ–‡æœ¬é¢„å¤„ç†ï¼ˆå’Œè®­ç»ƒæ—¶ç¼–ç é€»è¾‘ä¸€è‡´ï¼‰
    src_tokens = src_tokenizer.encode(
        src_text,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        return_tensors="pt"
    ).to(device)
    src_mask = create_padding_mask(src_tokens, pad_idx, device)

    # 2. ç¼–ç æºæ–‡æœ¬ï¼ˆç¦ç”¨æ¢¯åº¦ï¼ŒåŠ é€Ÿæ¨ç†ï¼‰
    model.eval()
    with torch.no_grad():
        encoder_output = model.encode(src_tokens, src_mask)

    # 3. åˆå§‹åŒ–ç›®æ ‡åºåˆ—ï¼ˆä»¥bos tokenå¼€å¤´ï¼‰
    tgt_bos_token = tgt_tokenizer.bos_token_id
    tgt_eos_token = tgt_tokenizer.eos_token_id
    generated_tgt = torch.tensor([[tgt_bos_token]], device=device)

    # 4. è‡ªå›å½’ç”Ÿæˆï¼ˆé€è¯é¢„æµ‹ï¼‰
    for _ in range(max_gen_len - 1):
        tgt_seq_len = generated_tgt.size(1)
        # å› æœæ©ç ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥è¯
        tgt_mask = create_causal_mask(tgt_seq_len, device).unsqueeze(0)
        # paddingæ©ç ï¼šå±è”½pad token
        tgt_pad_mask = create_padding_mask(generated_tgt, pad_idx, device)

        with torch.no_grad():
            # è§£ç é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
            output = model.decode(
                tgt=generated_tgt,
                encoder_output=encoder_output,
                tgt_mask=tgt_mask,
                memory_mask=src_mask
            )
            # æŠ•å½±åˆ°è¯æ±‡è¡¨ï¼Œé€‰æ¦‚ç‡æœ€é«˜çš„è¯ï¼ˆè´ªå¿ƒæœç´¢ï¼‰
            next_token_logits = model.output_projection(output[:, -1, :])
            next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)

        # è¿½åŠ åˆ°ç›®æ ‡åºåˆ—
        generated_tgt = torch.cat([generated_tgt, next_token_id], dim=-1)
        # ç”Ÿæˆeos tokenåˆ™ç»ˆæ­¢
        if next_token_id.item() == tgt_eos_token:
            break

    # 5. è§£ç ä¸ºè‡ªç„¶æ–‡æœ¬ï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼‰
    translated_text = tgt_tokenizer.decode(
        generated_tgt.squeeze().cpu().numpy(),
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    )
    return translated_text


def main():
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæå‰æŠ¥é”™ï¼Œé¿å…åç»­éº»çƒ¦ï¼‰
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(
            f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼è¯·ç¡®è®¤è·¯å¾„ï¼š{os.path.abspath(MODEL_PATH)}\n"
            f"å½“å‰ç›®å½•ï¼š{os.getcwd()}\n"
            f"é¢„æœŸç›®å½•ç»“æ„ï¼šTransformer-Seq2Seq/src/ â†’ åŒ…å« infer.py å’Œ checkpoints/ æ–‡ä»¶å¤¹"
        )

    # 1. åŠ è½½tokenizerï¼ˆå¤ç”¨data.pyé…ç½®ï¼Œç¡®ä¿ç¼–ç ä¸€è‡´ï¼‰
    print("âœ… åŠ è½½tokenizer...")
    _, _, src_tokenizer, tgt_tokenizer, src_vocab_size, tgt_vocab_size, pad_idx = get_dataloaders(
        batch_size=32,
        max_len=MAX_SEQ_LEN,
        num_workers=0
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‚æ•°ä¸train.pyå®Œå…¨å¯¹é½ï¼‰
    print("âœ… åˆå§‹åŒ–æ¨¡å‹...")
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS,
        d_ff=D_FF,
        max_seq_len=MAX_SEQ_LEN,
        dropout=DROPOUT,
        pad_idx=pad_idx
    )

    # 3. åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå…³é”®ä¿®æ”¹ï¼šåªå–model_state_dictéƒ¨åˆ†ï¼‰
    print(f"âœ… åŠ è½½æ¨¡å‹æƒé‡ï¼š{MODEL_PATH}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ è¿è¡Œè®¾å¤‡ï¼š{device}")
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    # åˆ¤æ–­æ˜¯å¦æ˜¯å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼ˆå«epochç­‰ï¼‰ï¼Œå¦‚æœæ˜¯åˆ™å–model_state_dictï¼Œå¦åˆ™ç›´æ¥åŠ è½½
    if "model_state_dict" in checkpoint.keys():
        model.load_state_dict(checkpoint["model_state_dict"], strict=True)
    else:
        model.load_state_dict(checkpoint, strict=True)
    model.to(device)
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropout

    # 4. æ‰§è¡Œç¿»è¯‘å¹¶è¾“å‡ºç»“æœ
    print(f"\n=== ğŸš€ ç¿»è¯‘ç»“æœ ===")
    print(f"è¾“å…¥å¾·è¯­ï¼š{TEST_SRC_TEXT}")
    translated_text = translate_text(
        model=model,
        src_text=TEST_SRC_TEXT,
        src_tokenizer=src_tokenizer,
        tgt_tokenizer=tgt_tokenizer,
        pad_idx=pad_idx,
        max_gen_len=MAX_GEN_LEN,
        device=device
    )
    print(f"ç”Ÿæˆè‹±è¯­ï¼š{translated_text}")
    print(f"=== ğŸ‰ æ¨ç†å®Œæˆ ===")


if __name__ == "__main__":
    main()